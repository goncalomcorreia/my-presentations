%! TeX program = xelatex

\documentclass[xetex,aspectratio=169,xcolor,professionalfonts,hyperref]{beamer}
\usepackage[
    style=authoryear,
    backend=biber,
    natbib,
    uniquename=false,
    maxcitenames=2,
    maxbibnames=20,
    sorting=nty]{biblatex}
\addbibresource{refs.bib}
\renewcommand*{\bibfont}{\scriptsize}
\usepackage{anyfontsize}
\usepackage{tabularx}
\usepackage{changepage}
\input{preamble}
\input{commands}
\captionsetup[figure]{labelformat=empty}
\pgfplotsset{compat=1.16}
\newcommand\simplex{\triangle}
\newcommand\HHs{\HH^{\textsc{s}}}
\newcommand\HHg{\HH^{\textsc{g}}}
\newcommand\HHta{\HH^{\textsc{t}}_{\alpha}}
\newcommand\xv{\bs{x}}
\newcommand{\matr}[1]{\mathbf{#1}}
\DeclareMathOperator{\softmax}{\mathbf{softmax}}
\DeclareMathOperator{\sparsemax}{\mathbf{sparsemax}}
\definecolor{tBleu}{RGB}{118,169,196}
\definecolor{tDY}{RGB}{217,192,102}
\title{Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity}
\author{Gonçalo Correia, Vlad Niculae, Andr\'{e} F.T. Martins}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

% \AtBeginSection[]
% {
% \begin{frame}
%     \frametitle{Table of Contents}

%     \begin{adjustwidth}{5cm}{0.1em}
%         \tableofcontents[currentsection]
%     \end{adjustwidth}
% \end{frame}
% }

\begin{document}

\begin{frame}
%\titlepage
\begin{tikzpicture}[remember picture, overlay]

\node[font={\color{myfg}\usebeamerfont{title}},align=center]
    at ($(current page.center) + (0, 2.5)$) {\color{myDarkYellow} Efficient Marginalization};
\node[font={\color{myfg}\usebeamerfont{title}},align=center]
    at ($(current page.center) + (0, 1.8)$) {\color{myDarkYellow} of Discrete and Structured Latent Variables};
\node[font={\color{myfg}\usebeamerfont{title}},align=center]
    at ($(current page.center) + (0, 1.0)$) {\color{myDarkYellow} via Sparsity};
\node[anchor=north,font={\color{myfg}\usebeamerfont{author}}]
    at ($(current page.center) + (0, 0.5)$)
{
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{r l}
\textbf{Gonçalo Correia} &\small \textcolor{mygr}{Instituto de Telecomunicações, Lisbon} \\
Vlad Niculae             &\small \textcolor{mygr}{IvI, University of Amsterdam} \\
Wilker Aziz             &\small \textcolor{mygr}{ILLC, University of Amsterdam} \\
André Martins            &\small \textcolor{mygr}{Instituto de Telecomunicações \& LUMLIS \& Unbabel}  \\
\end{tabular}
};

\node[anchor=south,font={\color{mygr}\footnotesize}]
    at (current page.south)
{
\raisebox{-0.7mm}[\height][\depth]{\emoji{githubfg}} \href{https://github.com/deep-spin/sparse-marginalization-lvm}{\tt github.com/deep-spin/sparse-marginalization-lvm}
\quad
\raisebox{-0.4mm}[\height][\depth]{\emoji{home}}
\href{https://goncalomcorreia.github.io}{\tt goncalomcorreia.github.io}
};
\end{tikzpicture}
\end{frame}

\tikzset{%
    enc/.style={fill=tPurple!80!mybg},
    attn/.style={fill=tPeony},
    dec/.style={fill=tBlue!50!mybg},
    wvec/.style={
        inner sep=0,
        rectangle,
        rounded corners=2pt,
        minimum width=5pt,
        minimum height=18pt},
    word/.style={
        color=mygr,
        font=\itshape
    },
    netarrow/.style={->, color=mygr},
    attnedge/.style={tPeony, thick}
}

\section{Context}

\begin{frame}
    \frametitle{Latent Variable Models}

    \begin{itemize}
        \uncover<1->{\item[] Latent variable $z$ can be }\uncover<2->{{\color{tGreen} continuous}}\uncover<3->{, {\color{tPeony} discrete}}\uncover<4->{, or {\color{tVividBlue} structured}}
    \end{itemize}
    
    \only<1-2>{\uncover<2>{
    \begin{figure}[hb]
        \centering
        \begin{subfigure}[b]{0.24\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/face1.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.24\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/face2.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.24\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/face3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.24\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/face4.png}
        \end{subfigure}
        \caption{Source: \cite{faces}}
        \label{fig:rotation}
    \end{figure}
    }}

    \only<3>{
    \begin{figure}[hb]
        \centering
            \includegraphics[width=0.5\columnwidth]{figures/eyes.png}
            \caption{Source: \href{https://valleyeyecareaz.com/how-is-your-eye-color-determined/}{valleyeyecareaz.com}}
        \label{fig:eyes}
    \end{figure}
    }

    \only<4>{
    \begin{figure}[hb]
        \centering
            \includegraphics[width=0.6\columnwidth]{figures/celeba_bg.pdf}
            \caption{Source: \cite{liu2015faceattributes}}
        \label{fig:eyes}
    \end{figure}
    }

\end{frame}

\begin{frame}
    \frametitle{Training Discrete or Structured Latent Variable Models}
    \fontsize{12pt}{15}\selectfont

    \begin{columns}
    \hspace{2mm}\vspace{-1cm}\begin{column}{0.7\columnwidth}\vspace{-1cm}
    \begin{itemize}
        \uncover<1->{\item[] Latent variable $z$ can be }\uncover<2->{{\color{tPeony} discrete}}\uncover<3->{ or {\color{tVividBlue} structured}}
    \end{itemize}

    \begin{itemize}
        \uncover<4->{\item[] $\pi(z | x, \theta)$: distribution over possible $z$}
    \end{itemize}

    % besides this, there's also
    \begin{itemize}
        \uncover<7->{\item[] $\ell(x, z; \theta)$: downstream loss: ELBO, Log-Likelihood, (...)}
    \end{itemize}

    \end{column}

    \begin{column}{0.25\columnwidth}
            \vspace{-0.5cm}
            \begin{center}
                \begin{figure}[ht]
                \begin{tikzpicture}
                    % DISCRETE
                    \uncover<2->{\draw[draw=tPink,fill=tPink] (1.4,2) circle (0.2) node[anchor=south, yshift=2mm] {{\visible<5->{\color{tPeony} \small 0.2}}};}
                    \uncover<2->{\draw[draw=tSlateBlue,fill=tSlateBlue] (2,2) circle (0.2) node[anchor=south, yshift=2mm] {{\visible<5->{\color{tPeony} \small 0.6}}};}
                    \uncover<2->{\draw[draw=tGreen,fill=tGreen] (2.6,2) circle (0.2) node[anchor=south, yshift=2mm] {{\visible<5->{\color{tPeony} \small 0.1}}};}

                    % STRUCTURE
                    \uncover<3->{\draw[draw=tSlateBlue,fill=tSlateBlue] (1.4,1) circle (0.2) node[anchor=east, xshift=-2mm] {$[$};}
                    \uncover<3->{\draw[draw=tGreen,fill=tGreen] (2,1) circle (0.2);}
                    \uncover<3->{\draw[draw=tPink,fill=tPink] (2.6,1) circle (0.2)
                        node[anchor=west, xshift=2mm] {$]$}
                        node[anchor=west, xshift=5mm] {{\visible<6->{\color{tVividBlue} \small 0.4}}};}

                    \uncover<3->{\draw[draw=tPink,fill=tPink] (1.4,0.5) circle (0.2) node[anchor=east, xshift=-2mm] {$[$};}
                    \uncover<3->{\draw[draw=tSlateBlue,fill=tSlateBlue] (2,0.5) circle (0.2) node[anchor=north, yshift=-4mm] {\large \bf $\ldots$};}
                    \uncover<3->{\draw[draw=tGreen,fill=tGreen] (2.6,0.5) circle (0.2)
                        node[anchor=west, xshift=2mm] {$]$}
                        node[anchor=west, xshift=5mm] {{\visible<6->{\color{tVividBlue} \small 0.05}}};}

                    \uncover<3->{\draw[draw=tGreen,fill=tGreen] (1.4,-0.5) circle (0.2) node[anchor=east, xshift=-2mm] {$[$};}
                    \uncover<3->{\draw[draw=tSlateBlue,fill=tSlateBlue] (2,-0.5) circle (0.2);}
                    \uncover<3->{\draw[draw=tPink,fill=tPink] (2.6,-0.5) circle (0.2)
                        node[anchor=west, xshift=2mm] {$]$}
                        node[anchor=west, xshift=5mm] {{\visible<6->{\color{tVividBlue} \small 0.3}}};}
                \end{tikzpicture}
                \end{figure}
            \end{center}
    \end{column}
    \end{columns}

    \vspace{-0.5cm}

    \begin{itemize}
        \uncover<8->{\item[] To train, we need to compute the following expectation:}
    \end{itemize}

    \begin{equation*}\label{eq:fit}
        \uncover<9->{\mathcal{L}_{x}(\theta) =
        \sum_{z \in \mathcal Z}
        \pi(z | x, \theta)
        ~\ell(x, z; \theta)}
    \end{equation*}

    \begin{itemize}
        \uncover<10->
        {\item[] If $\mathcal Z$ is
        \only<10>{{\color{tPeony} large}, this sum can get very expensive due to $\ell(x, z; \theta)$!\quad\emoji{oface}}
        \only<11->{{\color{tVividBlue} combinatorial}, this can be intractable to compute!\quad\emoji{oface}\enspace\emoji{oface}\enspace\emoji{oface}}}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Current Solutions}
    \fontsize{12pt}{15}\selectfont

    \begin{itemize}
        \item[] If $\mathcal Z$ is large, exact gradient computation is prohibitive
    \end{itemize}

    \bigskip

    \begin{itemize}
        \uncover<2->{\item[] One option: SFE (aka REINFORCE)---unbiased but high variance}
        \uncover<3->{\item[] Another option: Gumbel-Softmax---continuous relaxation, biased estimation}
    \end{itemize}

    \bigskip

    \begin{itemize}
        \uncover<4->{\item[] New option: {\color{tPeony} use sparsity}!\quad\emoji{palms}}
    \end{itemize}

    \begin{itemize}
        \uncover<5->{\item[] no need for sampling --> no variance}
        \uncover<6->{\item[] no relaxation into the continuous space}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Taking a step back...}
    \fontsize{14pt}{15}\selectfont
    \begin{itemize}
        \item[] Does the expectation over possible $z$ need to be expensive?
    \end{itemize}

    \begin{align*}\label{eq:fit}
        \uncover<2->{\mathcal{L}_{x}(\theta) &=
        \sum_{z \in \mathcal Z}
        \pi(z | x, \theta)~\ell(x, z; \theta) \\&=
        \pi(z_1 | x, \theta)~\ell(x, z_1; \theta) + \pi(z_2 | x, \theta)~\ell(x, z_2; \theta) + \ldots \\&+ \pi(z_i | x, \theta)~\ell(x, z_i; \theta) + \ldots + \pi(z_N | x, \theta)~\ell(x, z_N; \theta)
        }
    \end{align*}

    % \begin{itemize}
    %     \uncover<3->{\item[] If components of $\pi(z | x, \theta)$ were exactly $0$, we could skip lots of computations!}
    % \end{itemize}

    \begin{itemize}
        \uncover<3->{\item[] Usually we normalize $\pi$ with softmax$\propto \exp(\pi) \Rightarrow \pi(z_i | x, \theta)>0$}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Sparse normalizers}
    \fontsize{12pt}{15}\selectfont
    \cornercite[north east]{sparsemax, sparsemap}
    \begin{itemize}
        \item[] We use {\color{tPeony} sparsemax}, {\color{tVividBlue} top-$k$ sparsemax} and {\color{tVividBlue} SparseMAP} to allow efficient marginalization
    \end{itemize}

    \begin{itemize}
        \uncover<2->{\item[] These functions are able to assign {\bf probabilities of exactly zero}!}
    \end{itemize}

    \begin{align*}
        \uncover<3->{\mathcal{L}_{x}(\theta) &=
        \sum_{z \in \mathcal Z}
        \pi(z | x, \theta)~\ell(x, z; \theta) \\&=
        \pi(z_1 | x, \theta)~\ell(x, z_1; \theta) + \alt<3>{\pi(z_2 | x, \theta)~\ell(x, z_2; \theta)}{\cancel{\pi(z_2 | x, \theta)~\ell(x, z_2; \theta)}} + \ldots \\&+
        \pi(z_i | x, \theta)~\ell(x, z_i; \theta) + \ldots + \alt<3>{\pi(z_N | x, \theta)~\ell(x, z_N; \theta)}{\cancel{\pi(z_N | x, \theta)~\ell(x, z_N; \theta)}}
        }
    \end{align*}

    \begin{itemize}
        \uncover<4->{\item[] No need for computing $\ell(x, z; \theta)$ for all $z \in \mathcal Z$!}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results}
    \fontsize{14pt}{15}\selectfont
    \begin{itemize}
        \uncover<1->{\item[] We test our methods for models with discrete latent variables,}
        \begin{itemize}
            \uncover<2->{\item Semi-Supervised VAE}
            \uncover<3->{\item Emergent communication}
        \end{itemize}
        \uncover<4->{\item[] but also in models with an exponentially large set of $\mathcal Z$,}
        \begin{itemize}
            \uncover<5->{\item Bit-vector VAE}
        \end{itemize}
    \end{itemize}

    \begin{itemize}
        \uncover<6->{\item[] Our methods are top-performers and efficient!}
    \end{itemize}
\end{frame}

\begin{frame}
    %\titlepage
    \begin{tikzpicture}[remember picture, overlay]
    
    \node[font={\color{myfg}\usebeamerfont{title}},align=center]
        at ($(current page.center) + (0, 2.5)$) {\color{myDarkYellow} Efficient Marginalization};
    \node[font={\color{myfg}\usebeamerfont{title}},align=center]
        at ($(current page.center) + (0, 1.8)$) {\color{myDarkYellow} of Discrete and Structured Latent Variables};
    \node[font={\color{myfg}\usebeamerfont{title}},align=center]
        at ($(current page.center) + (0, 1.0)$) {\color{myDarkYellow} via Sparsity};
    \node[anchor=north,font={\color{myfg}\usebeamerfont{author}}]
        at ($(current page.center) + (0, 0.5)$)
    {
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{r l}
    \textbf{Gonçalo Correia} &\small \textcolor{mygr}{Instituto de Telecomunicações, Lisbon} \\
    Vlad Niculae             &\small \textcolor{mygr}{IvI, University of Amsterdam} \\
    Wilker Aziz             &\small \textcolor{mygr}{ILLC, University of Amsterdam} \\
    André Martins            &\small \textcolor{mygr}{Instituto de Telecomunicações \& LUMLIS \& Unbabel}  \\
    \end{tabular}
    };
    
    \node[anchor=south,font={\color{mygr}\footnotesize}]
        at (current page.south)
    {
    \raisebox{-0.7mm}[\height][\depth]{\emoji{githubfg}} \href{https://github.com/deep-spin/sparse-marginalization-lvm}{\tt github.com/deep-spin/sparse-marginalization-lvm}
    \quad
    \raisebox{-0.4mm}[\height][\depth]{\emoji{home}}
    \href{https://goncalomcorreia.github.io}{\tt goncalomcorreia.github.io}
    };
    \end{tikzpicture}
\end{frame}

% \begin{frame}
%     \frametitle{
%         \only<7->{{\color{myDarkYellow}Adaptively}} \only<5->{{\color{colorEntmax}Sparse}} \uncover<1->{Transformers}}

% \only<1-4>{
%     \fontsize{12pt}{15}\selectfont
%     \cornercite{transf}
%     \begin{columns}
%     \hspace{2mm}\vspace{-1cm}\begin{column}{0.55\columnwidth}
%     In each attention head:
%     \begin{equation*}
%     \bar{\matr{V}}  = \softmax\left(\frac{\matr{Q}\matr{K}^\top}{\sqrt{d_k}}\right)\matr{V}.
%     \end{equation*}
%     \uncover<2-4>{Attention in three places:
%     \begin{itemize}
%     \item Self-attention in the encoder\tikz[remember picture]{\node[coordinate] (n1) {};}}
%     \uncover<3-4>{\item Self-attention in the decoder\tikz[remember picture]{\node[coordinate] (n2) {};}}
%     \uncover<4>{\item Contextual attention\tikz[remember picture]{\node[coordinate] (n3) {};}}
%     \end{itemize}

%     % \vspace{-0.7cm}
%     % \begin{align*}
%     %     \uncover<2-4>{6 \text{ layers } \times 8 \text{ attention heads} &= 48}
%     %     \\\uncover<3-4>{&+48}\\\only<4>{&+48=144\text{ attention heads}}
%     % \end{align*}

%     \end{column}
%     \begin{column}{0.4\columnwidth}
%     \vspace{-1.5cm}
%     \begin{center}
%     \includegraphics[width=0.9\columnwidth]{figures/transformer_mybg}
%     \tikz[baseline,remember picture]{\node[anchor=base] (t1){};}
%     \end{center}
%     \end{column}
%     \end{columns}

%     \begin{tikzpicture}[remember picture,overlay]   %% use here too
%         \uncover<2>{\path[draw=magenta,ultra thick,->](
%             [xshift=2mm,yshift=1mm]n1.north) to [out=6cm,in=0,distance=-1.5cm] ([xshift=-5.13cm,yshift=2.0cm]t1.north);}
%         \uncover<3>{\path[draw=magenta,ultra thick,->](
%             [xshift=2mm,yshift=1mm]n2.north) to [out=6cm,in=0,distance=-3cm] ([xshift=-2.67cm,yshift=2.0cm]t1.north);}
%         \uncover<4>{\path[draw=magenta,ultra thick,->](
%             [xshift=2mm,yshift=1mm]n3.north) to [out=-6cm,in=0,distance=-2.5cm] ([xshift=-2.67cm,yshift=3.55cm]t1.north);}
%     \end{tikzpicture}
% }

% \begin{itemize}
% \item[]\uncover<6->{
%     {\color{colorEntmax} Key idea:} replace softmax in attention heads by a sparse normalizing function! \quad\emoji{palms}
% }

% \bigskip

% \item[]\uncover<7->{
%     {\color{myDarkYellow} Another key idea:}
%     use a normalizing function that is adaptively sparse via a learnable $\alpha$! \quad\emoji{palms}\enspace\emoji{palms}\enspace\emoji{palms}
% }
% \end{itemize}

% % \bigskip

% % \begin{itemize}
% % \uncover<4->{\item Recall: $\alpha$ controls propensity to sparsity}
% % \uncover<5->{\item Learn each $\alpha \in [1,2]$ {\bf adaptively}!}
% % \uncover<6->{\item One $\alpha$ for each attention head and each layer}
% % \uncover<7->{\item Heads can be dense or sparse, depending on their roles.}
% % \end{itemize}

% \end{frame}

% \begin{frame}[fragile]
%     \frametitle{Related Work: Other Sparse Transformers}
%     \cornercite{Child2019,Sukhbaatar2019}

%     \vspace{-1.5cm}
%     \begin{center}
%     \includegraphics[width=0.7\columnwidth]{figures/comparison_mybg}

%     \bigskip

%     Our model allows {\color{myDarkYellow} non-contiguous} attention for each head.
%     \end{center}

% \end{frame}

% \section{Sparse Transformations}

% \begin{frame}[plain,t,fragile]%
%     \frametitle{What is softmax?}%
%     \centering \fontsize{12pt}{15}\selectfont
%     Softmax exponentiates and normalizes:\quad
%     $\displaystyle
%     \softmax(\xx_i) \defeq \frac{\exp \left(\xx_i\right)}{\sum_j \exp \left(\xx_j\right)}$

%     \uncover<2->{
%     {\color{myDarkYellow} It's fully dense: $\softmax(\vectsymb{z}) > \vect{0}$}}

%     \vspace{1cm}

%     \uncover<3->{Argmax can be written as:\\
%     \vspace{0.5cm}
%     $\displaystyle
%     \argmaxbf(\vectsymb{z}) \defeq \arg\max_{\vectsymb{p} \in \triangle} \DP{\vectsymb{z}}{\vectsymb{p}}$

%     \bigskip

%     \begin{itemize}
%     \item<4-> Retrieves a {\bf one-hot vector} for the highest scored index.
%     \item<5-> Sometimes used as hard attention, but not differentiable!
%     \end{itemize}
%     }
% \end{frame}

% \begin{frame}{$\Omega$-Regularized Argmax}
%     \cornercite{Niculae2017}
%     \fontsize{12pt}{15}\selectfont
%     \vspace{-0.5cm}
%     \begin{itemize}
%     \item[] For convex $\Omega$, define the {\bf $\Omega$-regularized argmax transformation}:\\
%     \bigskip
%     \begin{center}
%     $\displaystyle
%     \argmaxbf{}_{{\Omega}}(\vectsymb{z}) \defeq \arg\max_{\vectsymb{p} \in \triangle} \DP{\vectsymb{z}}{\vectsymb{p}} {\color{tPeony}- \Omega(\vectsymb{p})}$
%     \end{center}
%     \end{itemize}
%     \bigskip
%     \begin{itemize}
%     \uncover<2->{\item {\color{myDarkYellow} Argmax} corresponds to {\bf no regularization}, $\displaystyle\Omega \equiv 0$}
%     \uncover<3->{\item {\color{myDarkYellow} Softmax} amounts to {\bf entropic regularization}, $\displaystyle\Omega(\vectsymb{p}) = \sum_{i=1}^K p_i \log p_i$}
%     \uncover<4->{\item {\color{myDarkYellow} Sparsemax} amounts to {\bf $\ell_2$-regularization}, $\displaystyle\Omega(\vectsymb{p}) = \frac{1}{2}\|\vectsymb{p}\|^2$.}
%     \end{itemize}
%     \bigskip
%     \begin{itemize}
%     \item[] \uncover<5->{Is there something in-between?}
%     \end{itemize}
%     \uncover<4>{\cornercite[south east]{sparsemax}}
% \end{frame}

% \begin{frame}{$\alpha$-Entmax}
%     \cornercite{Peters2019ACL}
%     \vspace{-1cm}
%     \fontsize{12pt}{15}\selectfont
%     \begin{itemize}
%     \item[] Parametrized by {\color{tPeony}$\alpha \ge 0$}:
%     \end{itemize}
%     \bigskip
%     \begin{center}
%     $\displaystyle
%     \Omega_{{\color{tPeony}\alpha}}(\vectsymb{p}) \defeq 
%     \left\{
%     \begin{array}{ll}
%     \frac{1}{\alpha(\alpha-1)} \left(1 - \sum_{i=1}^K p_i^{\alpha}\right) & \text{if $\alpha \ne 1$}\\
%     \sum_{i=1}^K p_i\log p_i & \text{if $\alpha = 1$.}
%     \end{array}
%     \right.$
%     \end{center}
%     \bigskip
%     \begin{itemize}
%         \uncover<2->{\item {\bf Argmax} corresponds to {\color{tPeony}$\alpha \rightarrow \infty$}}
%         \uncover<3->{\item {\bf Softmax} amounts to {\color{tPeony}$\alpha \rightarrow 1$}}
%         \uncover<4->{\item {\bf Sparsemax} amounts to {\color{tPeony}$\alpha = 2$}.}
%     \end{itemize}
%     \bigskip
%     \begin{itemize}
%         \uncover<5->{\item[] {\color{myDarkYellow} Key result:} {\bf can be sparse for $\alpha > 1$}, propensity for sparsity increases with $\alpha$.}
%     \end{itemize}

% \end{frame}

% \section{Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity}

% \begin{frame}
%     \frametitle{Learning $\alpha$}

%     \begin{itemize}
%         \uncover<2->{\item[] {\color{myDarkYellow} Key contribution}: \\\bigskip\quad a closed-form expression for $\pfrac{\aentmax(\x)}{\alpha}$ \quad\emoji{oface}}
%     \end{itemize} 

%     \bigskip

%     \begin{itemize}

%         \uncover<3->{\item[] Requires argmin differentiation $\rightarrow$ see paper for details!}

%     \end{itemize}

%     \uncover<4->{\overlaybox[0.5]{\texttt{:pip install entmax}\\Check \href{https://github.com/deep-spin/entmax}{\tt github.com/deep-spin/entmax}}}

% \end{frame}

% \begin{frame}
%     \frametitle{BLEU Scores}

%     \begin{table}[ht]
%         \begin{center}
%         \small
%         \resizebox{0.8\columnwidth}{!}{\begin{tabular}{lrrrr}
%         \toprule
%         activation
%         & \langp{de}{en} & \langp{ja}{en}
%         & \langp{ro}{en} & \only<1>{\langp{en}{de}}\only<2->{{\color{tPeony} \langp{en}{de}}}\\
%         \midrule
%         $\softmaxlight$
%         & 29.79
%         & 21.57
%         & 32.70
%         & 26.02 \\
%         $\aentmax[1.5]$
%         & 29.83
%         & {\color{myDarkYellow} 22.13}
%         & {\color{myDarkYellow} 33.10}
%         & 25.89 \\
%         $\aentmax[\alpha]$
%         & {\color{myDarkYellow} 29.90}
%         & 21.74
%         & 32.89
%         & {\color{myDarkYellow} 26.93} \\
%         \bottomrule
%         \end{tabular}}
%         \end{center}
%     \end{table}

%     \bigskip

%     \begin{itemize}
%         \uncover<3>{\item[] For analysis for other language pairs, see Appendix A.}
%     \end{itemize}

% \end{frame}

% \section{Conclusions}

% \begin{frame}[fragile]
%   \frametitle{Key Takeaways}

%     \centering\fontsize{14pt}{14}\selectfont%
%     Introduce {\color{myDarkYellow} adaptive} sparsity\\
%     for Transformers via $\alpha$-entmax with a {\color{myDarkYellow}gradient learnable $\alpha$}.
%     %
%     %
%     \vfill
%     %
%     %
%     \begin{columns}[T]
%     \small
%     \begin{column}{.33\textwidth}
%     \centering
%     \uncover<2->{
%     \textbf{\emph{adaptive sparsity}}\\[.5\baselineskip]
%     \vspace{0.2cm}
%     \includegraphics[trim=157mm 17mm 0 0, clip, width=.7\textwidth]{figures/comparison_mybg}}%
%     \end{column}
%     \begin{column}{.33\textwidth}
%     \centering
%     \uncover<3->{
%     \textbf{\emph{reduced head redundancy}}\\[\baselineskip]}
%     \vspace{-0.2cm}
%     \begin{tikzpicture}[node distance=1.5ex,font=\scriptsize,scale=0.5, visible on=<3->]

%         \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
%         \definecolor{color1}{rgb}{1,0.498039215686275,0.0549019607843137}
%         \definecolor{color2}{rgb}{0.172549019607843,0.627450980392157,0.172549019607843}
        
%         \begin{groupplot}[group style={group size=1 by 1}]
%         \nextgroupplot[
%         legend cell align={left},
%         legend style={
%                 nodes={scale=1.1, transform shape}, at={(0.97,0.2)}, anchor=east, draw=white!80.0!black, fill=myfg!30!mybg},
%         tick align=outside,
%         tick pos=left,
%         x grid style={white!69.01960784313725!black},
%         xmin=0.5, xmax=6.5,
%         xtick = {1, 2, 3, 4, 5, 6},
%         xtick style={color=white},
%         y grid style={white!69.01960784313725!black},
%         ymin=0.1, ymax=0.7,
%         ytick = {0.2, 0.4, 0.6},
%         ytick style={color=white}
%         ]
%         \addplot [thick, color0, mark=square*, mark size=3, mark options={solid}]
%         table {%
%         1 0.38571667343747
%         2 0.402429158203537
%         3 0.440747738282957
%         4 0.359233941813858
%         5 0.337470844946825
%         6 0.339900884621234
%         };
%         \addlegendentry{softmax}
%         \addplot [thick, color1, mark=*, mark size=3, mark options={solid}]
%         table {%
%         1 0.378367748537659
%         2 0.504354104995477
%         3 0.573529792473815
%         4 0.525266398541884
%         5 0.439669581263257
%         6 0.421346772557364
%         };
%         \addlegendentry{1.5-entmax}
%         \addplot [thick, color2, mark=asterisk, mark size=3, mark options={solid}]
%         table {%
%         1 0.427742934860258
%         2 0.484287995253192
%         3 0.533714455762104
%         4 0.449772918584636
%         5 0.3935698561848
%         6 0.355665944457941
%         };
%         \addlegendentry{$\alpha$-entmax}
%     \end{groupplot}
%     \end{tikzpicture}
%     \end{column}
%     \begin{column}{.33\textwidth}
%     \centering
%     \uncover<4->{
%     \textbf{\emph{clearer head roles}}\\[\baselineskip]
%     \vspace{-0.2cm}
%     \includegraphics[width=.6\textwidth]{figures/bpe4}}
%     \end{column}
%     \end{columns}

%     \vfill

%     \centering
%     {\scriptsize
%     \color{mygr}
%     \begin{tabular}{r@{~}l@{\quad}r@{~}l}
%     \raisebox{-0.7mm}[\height][\depth]{\emoji{githubfg}}& \href{https://github.com/deep-spin/entmax}{\tt github.com/deep-spin/entmax} &
%     \raisebox{-0.4mm}[\height][\depth]{\emoji{home}}& \href{https://goncalomcorreia.github.io}{\tt goncalomcorreia.github.io}
%     \end{tabular}}

% \end{frame}

% \begin{frame}
%     \centering
%     \vspace{-2cm}
%     \fontsize{30pt}{15}\selectfont
%     Thank you!

%     \bigskip

%     \fontsize{20pt}{15}\selectfont
%     Questions?

%     \overlaybox[0.7]{\texttt{:pip install entmax}\\Check \href{https://github.com/deep-spin/entmax}{\tt github.com/deep-spin/entmax}}
% \end{frame}

% {
% \setbeamercolor{background canvas}{bg=white}
% \setbeamercolor{normal text}{fg=mygr}
% \setbeamercolor{frametitle}{fg=mybg}
% \usebeamercolor[fg]{frametitle}
% \usebeamercolor[fg]{normal text}
% \begin{frame}
% \frametitle{Acknowledgements}
% \centering
% \small
% \includegraphics[width=.2\textwidth]{img/erc.png}\\
% This work was supported by the European Research
% Council (ERC StG DeepSPIN 758969) and by the
% Fundação para a Ciência e Tecnologia through contract UID/EEA/50008/2019 and
% CMUPERI/TIC/0046/2014 (GoLocal).
% \end{frame}
% }

\begin{frame}[t,allowframebreaks]
\frametitle{References}
\printbibliography
\end{frame}

\end{document}

